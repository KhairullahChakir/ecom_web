{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 2: TabM Deep Learning Model Training\n",
                "\n",
                "This notebook implements the training pipeline for the **TabM** (Tableau Multi-Layer Perceptron) model. TabM is a state-of-the-art architecture designed specifically for tabular data, utilizing an efficient ensemble of parallel MLP heads.\n",
                "\n",
                "### üß† Model Overview:\n",
                "1. **Ensemble Architecture**: Uses a shared backbone with multiple parallel output heads to provide robust predictions.\n",
                "2. **PyTorch Implementation**: Leveraging GPU acceleration (if available) and the AdamW optimizer.\n",
                "3. **Comparison Against Baselines**: Directly benchmarks the results against the Random Forest and XGBoost models from Phase 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, roc_curve\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import joblib\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "# Premium Styling\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "PRIMARY_COLOR = '#1E4FA8'\n",
                "SECONDARY_COLOR = '#E8F0FF'\n",
                "\n",
                "# Device Configuration\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "# Paths\n",
                "DATA_PATH = \"../data/raw/online_shoppers_intention.csv\"\n",
                "MODELS_PATH = \"../backend/models\"\n",
                "REPORTS_PATH = \"../reports/metrics\"\n",
                "\n",
                "os.makedirs(MODELS_PATH, exist_ok=True)\n",
                "os.makedirs(REPORTS_PATH, exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Environment Ready! Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Advanced Data Loading\n",
                "We implement the same **80/10/10 split** used in the baseline to ensure a fair comparison. The data is then converted into PyTorch tensors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(DATA_PATH)\n",
                "\n",
                "# 1. Features Handling\n",
                "X = df.drop('Revenue', axis=1)\n",
                "y = df['Revenue'].astype(int)\n",
                "\n",
                "categorical_cols = ['Month', 'VisitorType', 'Weekend']\n",
                "numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
                "\n",
                "label_encoders = {}\n",
                "for col in categorical_cols:\n",
                "    le = LabelEncoder()\n",
                "    X[col] = le.fit_transform(X[col].astype(str))\n",
                "    label_encoders[col] = le\n",
                "\n",
                "# 2. 80/10/10 Split Strategy\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "X_test, X_val, y_test, y_val = train_test_split(\n",
                "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
                ")\n",
                "\n",
                "# 3. Scaling\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = X_train.copy()\n",
                "X_test_scaled = X_test.copy()\n",
                "X_val_scaled = X_val.copy()\n",
                "\n",
                "X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
                "X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
                "X_val_scaled[numerical_cols] = scaler.transform(X_val[numerical_cols])\n",
                "\n",
                "# 4. PyTorch Conversion\n",
                "X_train_t = torch.FloatTensor(X_train_scaled.values).to(DEVICE)\n",
                "X_val_t = torch.FloatTensor(X_val_scaled.values).to(DEVICE)\n",
                "X_test_t = torch.FloatTensor(X_test_scaled.values).to(DEVICE)\n",
                "y_train_t = torch.FloatTensor(y_train.values).to(DEVICE)\n",
                "y_val_t = torch.FloatTensor(y_val.values).to(DEVICE)\n",
                "y_test_t = torch.FloatTensor(y_test.values).to(DEVICE)\n",
                "\n",
                "print(f\"‚úÖ Data Split Complete!\")\n",
                "print(f\"   ‚Üí Training:   {len(X_train_t)}\")\n",
                "print(f\"   ‚Üí Validation: {len(X_val_t)}\")\n",
                "print(f\"   ‚Üí Testing:    {len(X_test_t)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Architecture: TabM\n",
                "TabM (Tableau Multi-Layer Perceptron) consists of an ensemble of MLP blocks. Each block learns independently while sharing basic feature normalization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MLPBlock(nn.Module):\n",
                "    def __init__(self, in_features, hidden_dim, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(in_features, hidden_dim)\n",
                "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
                "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
                "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.activation = nn.GELU()\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.activation(self.bn1(self.fc1(x)))\n",
                "        x = self.dropout(x)\n",
                "        x = self.activation(self.bn2(self.fc2(x)))\n",
                "        return x\n",
                "\n",
                "class TabMModel(nn.Module):\n",
                "    def __init__(self, n_features, hidden_dim=128, n_ensemble=8, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.input_bn = nn.BatchNorm1d(n_features)\n",
                "        self.ensemble_blocks = nn.ModuleList([\n",
                "            MLPBlock(n_features, hidden_dim, dropout) for _ in range(n_ensemble)\n",
                "        ])\n",
                "        self.output_heads = nn.ModuleList([\n",
                "            nn.Linear(hidden_dim, 1) for _ in range(n_ensemble)\n",
                "        ])\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.input_bn(x)\n",
                "        outputs = [head(block(x)) for block, head in zip(self.ensemble_blocks, self.output_heads)]\n",
                "        stacked = torch.stack(outputs, dim=0)\n",
                "        return stacked.mean(dim=0).squeeze(-1)\n",
                "\n",
                "print(\"‚úÖ Architecture Defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop\n",
                "Applying early stopping and cosine annealing for optimal convergence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = {\n",
                "    'hidden_dim': 128,\n",
                "    'n_ensemble': 4,\n",
                "    'dropout': 0.15,\n",
                "    'lr': 0.001,\n",
                "    'epochs': 100,\n",
                "    'batch_size': 256,\n",
                "    'patience': 15\n",
                "}\n",
                "\n",
                "model = TabMModel(X_train_t.shape[1], config['hidden_dim'], config['n_ensemble'], config['dropout']).to(DEVICE)\n",
                "criterion = nn.BCEWithLogitsLoss()\n",
                "optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=0.01)\n",
                "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
                "\n",
                "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=config['batch_size'], shuffle=True)\n",
                "\n",
                "history = {'train_loss': [], 'val_auc': []}\n",
                "best_val_auc = 0\n",
                "best_weights = None\n",
                "patience_counter = 0\n",
                "\n",
                "for epoch in range(config['epochs']):\n",
                "    # Training Mode\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    for X_batch, y_batch in train_loader:\n",
                "        optimizer.zero_grad()\n",
                "        preds = model(X_batch)\n",
                "        loss = criterion(preds, y_batch)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    \n",
                "    scheduler.step()\n",
                "    \n",
                "    # Validation Mode\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        val_preds = torch.sigmoid(model(X_val_t)).cpu().numpy()\n",
                "        val_auc = roc_auc_score(y_val_t.cpu().numpy(), val_preds)\n",
                "    \n",
                "    history['train_loss'].append(epoch_loss / len(train_loader))\n",
                "    history['val_auc'].append(val_auc)\n",
                "    \n",
                "    if val_auc > best_val_auc:\n",
                "        best_val_auc = val_auc\n",
                "        best_weights = model.state_dict().copy()\n",
                "        patience_counter = 0\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        \n",
                "    if (epoch + 1) % 10 == 0:\n",
                "        print(f\"Epoch {epoch+1:03d} | Loss: {epoch_loss/len(train_loader):.4f} | Val AUC: {val_auc:.4f}\")\n",
                "    \n",
                "    if patience_counter >= config['patience']:\n",
                "        print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
                "        break\n",
                "\n",
                "model.load_state_dict(best_weights)\n",
                "print(f\"‚≠ê Best Val AUC: {best_val_auc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Performance Evaluation\n",
                "Comparing the TabM results with our traditional baseline models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    test_probs = torch.sigmoid(model(X_test_t)).cpu().numpy()\n",
                "    test_preds = (test_probs >= 0.5).astype(int)\n",
                "\n",
                "tabm_metrics = {\n",
                "    'model': 'TabM (Deep Learning)',\n",
                "    'auc_roc': float(roc_auc_score(y_test_t.cpu().numpy(), test_probs)),\n",
                "    'f1': float(f1_score(y_test_t.cpu().numpy(), test_preds)),\n",
                "    'precision': float(precision_score(y_test_t.cpu().numpy(), test_preds)),\n",
                "    'recall': float(recall_score(y_test_t.cpu().numpy(), test_preds))\n",
                "}\n",
                "\n",
                "# Visualize Training History\n",
                "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Loss', color='tab:red')\n",
                "ax1.plot(history['train_loss'], color='tab:red', label='Train Loss')\n",
                "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
                "\n",
                "ax2 = ax1.twinx()\n",
                "ax2.set_ylabel('AUC-ROC', color='tab:blue')\n",
                "ax2.plot(history['val_auc'], color='tab:blue', label='Val AUC')\n",
                "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
                "\n",
                "plt.title('TabM Training History')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Comparison Table\n",
                "We load the previous baseline results and add TabM to the leaderboard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(f\"{REPORTS_PATH}/baseline_comparison.json\", \"r\") as f:\n",
                "    comparison = json.load(f)\n",
                "\n",
                "comparison.append(tabm_metrics)\n",
                "df_comp = pd.DataFrame(comparison).set_index('model').sort_values('auc_roc', ascending=False)\n",
                "\n",
                "print(\"üèÜ Leaderboard updated with TabM!\")\n",
                "df_comp"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Archival\n",
                "Saving the trained weights and the full comparison report."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.save({\n",
                "    'state_dict': model.state_dict(),\n",
                "    'config': config,\n",
                "    'n_features': X_train_t.shape[1]\n",
                "}, f\"{MODELS_PATH}/tabm_model.pt\")\n",
                "\n",
                "with open(f\"{REPORTS_PATH}/full_model_comparison.json\", \"w\") as f:\n",
                "    json.dump(comparison, f, indent=4)\n",
                "\n",
                "print(\"‚úÖ Model and comparison report saved!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}