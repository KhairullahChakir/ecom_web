{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Baseline Model Training (Optimized & Balanced)\n",
                "\n",
                "This notebook implements the baseline training for the **OP-ECOM** project. It establishes a comparison between several traditional models on the UCI Online Shoppers Purchasing Intention dataset.\n",
                "\n",
                "### üíé Key Features:\n",
                "1. **80/10/10 Split**: Training (80%), Testing (10%), and Validation (10%).\n",
                "2. **Training Set Balancing**: Manual upsampling to ensure the AI learns from an equal number of buyers and non-buyers.\n",
                "3. **Premium Visuals**: Lapis Lazuli themed performance charts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import (\n",
                "    roc_auc_score, f1_score, precision_score, recall_score,\n",
                "    classification_report, confusion_matrix, roc_curve, precision_recall_curve\n",
                ")\n",
                "import xgboost as xgb\n",
                "import joblib\n",
                "import json\n",
                "from datetime import datetime\n",
                "from sklearn.utils import resample\n",
                "\n",
                "# Premium Styling (ŸÑÿßÿ¨Ÿàÿ±ÿØ€å)\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "PRIMARY_COLOR = '#1E4FA8'\n",
                "SECONDARY_COLOR = '#E8F0FF'\n",
                "\n",
                "# Paths\n",
                "DATA_PATH = \"../data/raw/online_shoppers_intention.csv\"\n",
                "MODELS_PATH = \"../backend/models\"\n",
                "REPORTS_PATH = \"../reports/metrics\"\n",
                "\n",
                "os.makedirs(MODELS_PATH, exist_ok=True)\n",
                "os.makedirs(REPORTS_PATH, exist_ok=True)\n",
                "\n",
                "print(\"‚úÖ Environment Ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Distribution\n",
                "We start by loading the UCI dataset and verifying the class imbalance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(DATA_PATH)\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "\n",
                "# Target distribution visualization\n",
                "plt.figure(figsize=(8, 5))\n",
                "sns.countplot(x='Revenue', data=df, palette=[SECONDARY_COLOR, PRIMARY_COLOR])\n",
                "plt.title('Purchase Intent Distribution (Original)')\n",
                "plt.show()\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Advanced Preprocessing\n",
                "Implementing the 80/10/10 split and balancing the training set via upsampling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df.drop('Revenue', axis=1)\n",
                "y = df['Revenue'].astype(int)\n",
                "\n",
                "# 1. Features Handling\n",
                "categorical_cols = ['Month', 'VisitorType', 'Weekend']\n",
                "numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
                "\n",
                "label_encoders = {}\n",
                "for col in categorical_cols:\n",
                "    le = LabelEncoder()\n",
                "    X[col] = le.fit_transform(X[col].astype(str))\n",
                "    label_encoders[col] = le\n",
                "\n",
                "# 2. 80/10/10 Split Strategy\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "X_test, X_val, y_test, y_val = train_test_split(\n",
                "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
                ")\n",
                "\n",
                "# 3. Scaling\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = X_train.copy()\n",
                "X_test_scaled = X_test.copy()\n",
                "X_val_scaled = X_val.copy()\n",
                "\n",
                "X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
                "X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
                "X_val_scaled[numerical_cols] = scaler.transform(X_val[numerical_cols])\n",
                "\n",
                "# 4. Training Set Balancing (Oversampling)\n",
                "train_data = pd.concat([X_train_scaled, y_train], axis=1)\n",
                "not_buy = train_data[train_data.Revenue == 0]\n",
                "buy = train_data[train_data.Revenue == 1]\n",
                "\n",
                "buy_upsampled = resample(buy, replace=True, n_samples=len(not_buy), random_state=42)\n",
                "train_balanced = pd.concat([not_buy, buy_upsampled])\n",
                "\n",
                "X_train_final = train_balanced.drop('Revenue', axis=1)\n",
                "y_train_final = train_balanced.Revenue\n",
                "\n",
                "print(f\"‚úÖ Preprocessing Complete!\")\n",
                "print(f\"   ‚Üí Training: {len(X_train_final)} (Balanced)\")\n",
                "print(f\"   ‚Üí Test:     {len(X_test)}\")\n",
                "print(f\"   ‚Üí Val:      {len(X_val)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Multi-Model Training Pipeline\n",
                "We train baseline models and collect metrics for a comprehensive comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.1 Logistic Regression\n",
                "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
                "lr_model.fit(X_train_final, y_train_final)\n",
                "y_prob_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
                "y_pred_lr = lr_model.predict(X_test_scaled)\n",
                "\n",
                "# 3.2 XGBoost Classifier\n",
                "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=6, random_state=42, eval_metric='logloss')\n",
                "xgb_model.fit(X_train_final, y_train_final)\n",
                "y_prob_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
                "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
                "\n",
                "# 3.3 Decision Tree\n",
                "dt_model = DecisionTreeClassifier(random_state=42)\n",
                "dt_model.fit(X_train_final, y_train_final)\n",
                "y_prob_dt = dt_model.predict_proba(X_test_scaled)[:, 1]\n",
                "y_pred_dt = dt_model.predict(X_test_scaled)\n",
                "\n",
                "# 3.4 Random Forest\n",
                "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "rf_model.fit(X_train_final, y_train_final)\n",
                "y_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
                "y_pred_rf = rf_model.predict(X_test_scaled)\n",
                "\n",
                "# 3.5 SVM (Support Vector Machine)\n",
                "svm_model = SVC(probability=True, random_state=42)\n",
                "svm_model.fit(X_train_final, y_train_final)\n",
                "y_prob_svm = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
                "y_pred_svm = svm_model.predict(X_test_scaled)\n",
                "\n",
                "def get_metrics(name, y_true, y_pred, y_prob):\n",
                "    return {\n",
                "        'model': name,\n",
                "        'auc_roc': roc_auc_score(y_true, y_prob),\n",
                "        'f1': f1_score(y_true, y_pred),\n",
                "        'precision': precision_score(y_true, y_pred),\n",
                "        'recall': recall_score(y_true, y_pred)\n",
                "    }\n",
                "\n",
                "results = [\n",
                "    get_metrics('Logistic Regression', y_test, y_pred_lr, y_prob_lr),\n",
                "    get_metrics('XGBoost', y_test, y_pred_xgb, y_prob_xgb),\n",
                "    get_metrics('Decision Tree', y_test, y_pred_dt, y_prob_dt),\n",
                "    get_metrics('Random Forest', y_test, y_pred_rf, y_prob_rf),\n",
                "    get_metrics('SVM', y_test, y_pred_svm, y_prob_svm)\n",
                "]\n",
                "\n",
                "pd.DataFrame(results).set_index('model')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visual Comparison\n",
                "Let's see the performance spread and feature importance across models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 Combined ROC Curves\n",
                "plt.figure(figsize=(10, 7))\n",
                "probs = {\n",
                "    'Logistic Regression': y_prob_lr,\n",
                "    'XGBoost': y_prob_xgb,\n",
                "    'Decision Tree': y_prob_dt,\n",
                "    'Random Forest': y_prob_rf,\n",
                "    'SVM': y_prob_svm\n",
                "}\n",
                "\n",
                "for name, prob in probs.items():\n",
                "    fpr, tpr, _ = roc_curve(y_test, prob)\n",
                "    auc = roc_auc_score(y_test, prob)\n",
                "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.3f})\")\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('Baseline Model Performance: ROC Comparison')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "# 4.2 Feature Importance (Comparison)\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "importance_xgb = pd.Series(xgb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
                "sns.barplot(x=importance_xgb.values, y=importance_xgb.index, ax=ax1, palette='Blues_r')\n",
                "ax1.set_title('XGBoost Feature Importance')\n",
                "\n",
                "importance_rf = pd.Series(rf_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
                "sns.barplot(x=importance_rf.values, y=importance_rf.index, ax=ax2, palette='Blues_r')\n",
                "ax2.set_title('Random Forest Feature Importance')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# 5. Saving Results\n",
                "best_result = pd.DataFrame(results).set_index('model')['auc_roc'].idxmax()\n",
                "print(f\"‚≠ê Best Baseline Model: {best_result}\")\n",
                "\n",
                "# Save metrics\n",
                "with open(f\"{REPORTS_PATH}/baseline_comparison.json\", \"w\") as f:\n",
                "    json.dump(results, f, indent=4)\n",
                "\n",
                "# Save core components\n",
                "joblib.dump(xgb_model, f\"{MODELS_PATH}/xgb_baseline.joblib\")\n",
                "joblib.dump(rf_model, f\"{MODELS_PATH}/rf_baseline.joblib\")\n",
                "joblib.dump(lr_model, f\"{MODELS_PATH}/lr_baseline.joblib\")\n",
                "joblib.dump(scaler, f\"{MODELS_PATH}/scaler.joblib\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}